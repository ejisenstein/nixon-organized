{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evanisenstein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corextopic in /Users/evanisenstein/anaconda3/lib/python3.7/site-packages (1.0.4)\n",
      "Requirement already satisfied: networkx in /Users/evanisenstein/anaconda3/lib/python3.7/site-packages (2.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/evanisenstein/anaconda3/lib/python3.7/site-packages (from networkx) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "!pip install corextopic\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_df = pickle.load(open('complete_df.pickle', 'rb'))\n",
    "proba_df = pickle.load(open('proba_df.pickle', 'rb'))\n",
    "vf_df = pickle.load(open('vf_df.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cassette Number</th>\n",
       "      <th>Conversation Number</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Location_x</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Nixon Words</th>\n",
       "      <th>Participants</th>\n",
       "      <th>Full Dates</th>\n",
       "      <th>Full Transcript</th>\n",
       "      <th>files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E - 66</td>\n",
       "      <td>884-007</td>\n",
       "      <td>1973-03-20</td>\n",
       "      <td>White House Oval Office</td>\n",
       "      <td>4</td>\n",
       "      <td>What I think, what it really gets down to, bas...</td>\n",
       "      <td>President Nixon; H. R. (\"Bob\") Haldeman</td>\n",
       "      <td>MARCH 20, 1973</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E - 67</td>\n",
       "      <td>885-007</td>\n",
       "      <td>1973-03-20</td>\n",
       "      <td>White House Oval Office</td>\n",
       "      <td>46</td>\n",
       "      <td>Well, I'm trying to figure out with Ehrlichman...</td>\n",
       "      <td>President Nixon; H. R. (\"Bob\") Haldeman</td>\n",
       "      <td>MARCH 20, 1973</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E - 68</td>\n",
       "      <td>037-175</td>\n",
       "      <td>1973-03-20</td>\n",
       "      <td>White House Telephone</td>\n",
       "      <td>14</td>\n",
       "      <td>John Dean, please.  Hello.  I hear you're havi...</td>\n",
       "      <td>President Nixon; John W. Dean, III; White Hous...</td>\n",
       "      <td>MARCH 20, 1973</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E - 69</td>\n",
       "      <td>886-008</td>\n",
       "      <td>1973-03-21</td>\n",
       "      <td>White House Oval Office</td>\n",
       "      <td>111</td>\n",
       "      <td>John, sit down, sit down.  Well, what is the D...</td>\n",
       "      <td>President Nixon; John W, Dean, III; H. R. (\"Bo...</td>\n",
       "      <td>MARCH 20, 1973</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E - 72</td>\n",
       "      <td>421-018</td>\n",
       "      <td>1973-03-21</td>\n",
       "      <td>Old Executive Office Building Office</td>\n",
       "      <td>36</td>\n",
       "      <td>[Unintelligible] to this morning.  That's wher...</td>\n",
       "      <td>President Nixon; John W. Dean, III; H. R. (\"Bo...</td>\n",
       "      <td>MARCH 20, 1973</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cassette Number Conversation Number      Dates  \\\n",
       "0          E - 66             884-007 1973-03-20   \n",
       "1          E - 67             885-007 1973-03-20   \n",
       "2          E - 68             037-175 1973-03-20   \n",
       "3          E - 69             886-008 1973-03-21   \n",
       "4          E - 72             421-018 1973-03-21   \n",
       "\n",
       "                             Location_x Minutes  \\\n",
       "0               White House Oval Office       4   \n",
       "1               White House Oval Office      46   \n",
       "2                 White House Telephone      14   \n",
       "3               White House Oval Office     111   \n",
       "4  Old Executive Office Building Office      36   \n",
       "\n",
       "                                         Nixon Words  \\\n",
       "0  What I think, what it really gets down to, bas...   \n",
       "1  Well, I'm trying to figure out with Ehrlichman...   \n",
       "2  John Dean, please.  Hello.  I hear you're havi...   \n",
       "3  John, sit down, sit down.  Well, what is the D...   \n",
       "4  [Unintelligible] to this morning.  That's wher...   \n",
       "\n",
       "                                        Participants      Full Dates  \\\n",
       "0            President Nixon; H. R. (\"Bob\") Haldeman  MARCH 20, 1973   \n",
       "1            President Nixon; H. R. (\"Bob\") Haldeman  MARCH 20, 1973   \n",
       "2  President Nixon; John W. Dean, III; White Hous...  MARCH 20, 1973   \n",
       "3  President Nixon; John W, Dean, III; H. R. (\"Bo...  MARCH 20, 1973   \n",
       "4  President Nixon; John W. Dean, III; H. R. (\"Bo...  MARCH 20, 1973   \n",
       "\n",
       "                                     Full Transcript files  \n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...    34  \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...    35  \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...    36  \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...    37  \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...    38  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in pickled files try and run this tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function to Create SubDocuments\n",
    "Tokenized every fifteen sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transcript_to_doc(transcript):\n",
    "    \"\"\"Turns transcript \"\"\"\n",
    "    s = sent_tokenize(transcript) #seperates transcript into sentences\n",
    "    s = [x for x in s if len(x.split())>1] #make a list comprehension of sentences, sentence greater than one\n",
    "\n",
    "    s_fifteen = [] #empty list, meant to group each 15 sentences together\n",
    "    new_string = '' \n",
    "    counter = 0\n",
    "    for sent in s:\n",
    "        new_string += ' ' #this space seperates the end of sentences, so you dont get this (example.example)\n",
    "        new_string += sent\n",
    "        counter += 1\n",
    "        if counter % 15 == 0: #this should allow break condition, (less than 15 sentences) to be grouped as a final sentence\n",
    "            s_fifteen.append(new_string) #append string into s_15\n",
    "            new_string = '' \n",
    "\n",
    "    s_fifteen.append(new_string)\n",
    "            \n",
    "    return s_fifteen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Total Transcript List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### List of tokenized transcripts, ordered by transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a150e169dc8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nixon Words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlist_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_to_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9bd80d1e6efe>\u001b[0m in \u001b[0;36mtranscript_to_doc\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranscript_to_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Turns transcript \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#seperates transcript into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#make a list comprehension of sentences, sentence greater than one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "list_word = []\n",
    "for trans in wg_df['Nixon Words']:\n",
    "    list_word.append(transcript_to_doc(trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenized transcripts in one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_word = []\n",
    "for sub_list in list_word: \n",
    "    for item in sub_list:\n",
    "        total_word.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## List of Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_stop_words = stopwords.words('english')\n",
    "new_stops = ['unintelligible', 'uh', 'us', 'yeah', 'da', 'right',  'say', 'ready', 'think'\n",
    "             'well', 'let', 'would', 'see' , 'got', 'get', 'could', 'yes', 'john', 'pause'\n",
    "            'an', 'la', 'oh', 'says', 'ah', 'ah ah', 'like', 'really', 'going',\n",
    "            'go', 'gonna', 'know', 'uhh', 'well', 'think', 'thing', 'th', 'ya', 'um', 'hmm', 'hum',\n",
    "            'come', 'came', 'need', 'five', 'minutes', 'pause', 'sort', 'feel' ]\n",
    "my_stop_words.extend(new_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Corex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Count Vectorizer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub_docs = pd.Series(total_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=20000,\n",
    "                             stop_words= my_stop_words, \n",
    "                             token_pattern= '(?u)\\\\b\\\\w\\\\w+\\\\b',  #\"\\\\b[a-z][a-z]+\\\\b\" for just words\n",
    "                             #max_df = .9, #originally 1, ignore words that appear too often \n",
    "                             #min_df = .05, #ignore words that don't appear often enough\n",
    "                             binary=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_word = vectorizer.fit_transform(sub_docs)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Corex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_model_anch = ct.Corex(n_hidden=14, words=words, seed=42)\n",
    "topic_model_anch.fit(doc_word, words=words, docs=sub_docs,\n",
    "               anchors=[['money', 'dollars', 'million', 'thousand', 'cash', 'hunt'], #raised\n",
    "                       ['national', 'security', 'president', 'privileged'],\n",
    "                       ['house','white','jury','grand','staff','members',],\n",
    "                       ['executive','privilege','trial'],\n",
    "                       ['investigation','prosecutor','attorney','general','charge'],\n",
    "                       ['dean', 'told', 'ehrlichman', 'petersen', 'blackmail'],\n",
    "                       ['ervin', 'senate', 'committee', 'welcome'],\n",
    "                       ['pr', 'indicates', 'publicly'],\n",
    "                       ['plea', 'fifth', 'amendment' ],\n",
    "                       ['god', 'damn', 'son', 'bitch', 'hell'],\n",
    "                       ['obstruction', 'justice'],\n",
    "                       ['segretti', 'vulnerability'],\n",
    "                       ['fbi', 'gray', 'destroy', 'documents'],\n",
    "                        ['ellsberg', 'psychiatrist']        \n",
    "                       ], anchor_strength=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics = topic_model_anch.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ', '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_anch = pd.DataFrame(topic_model_anch.predict(doc_word), columns=['topic'+str(i) for i in range(14)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Transcript by Date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Top Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_docs(date, topic_num, option):\n",
    "    \n",
    "    #change date into datetime format (Y-m-d)\n",
    "    dt_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    #Filter dataframe by date\n",
    "    mask = proba_df['Dates'] == dt_date\n",
    "    subset_df = proba_df[mask]\n",
    "    \n",
    "    #organize dataframe by highest probability\n",
    "    organized_subset = subset_df.sort_values(by = topic_num, ascending=False)\n",
    "    \n",
    "    #return Sub_Doc row, depending on the option\n",
    "    return (organized_subset.iloc[option, 7], organized_subset.iloc[option, :]) #other idea\n",
    "    #return organized_subset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend Transcript by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_transcript_by_date(date, topic_num, option):\n",
    "    \n",
    "    #get subdocument transcript for requested topic\n",
    "    exact_sub_doc, meta_sub_doc = find_top_docs(date, topic_num, option) #other idea\n",
    "\n",
    "\n",
    "    mask = vf_df['Cassette Number'] == meta_sub_doc.loc['Cassette Number'] #mask to find matching cassette number\n",
    "    text = vf_df[mask].iloc[0, -2] # relevant transcript matched with cassette number \n",
    "    \n",
    "   \n",
    "    begin_index = 15\n",
    "    end_index = -15\n",
    "\n",
    "    \n",
    "    print('Cassette Number:  ' + vf_df[mask].iloc[0, 0]+ '\\n')\n",
    "    \n",
    "    try: \n",
    "        result = re.findall(exact_sub_doc[:begin_index] + '[.\\s\\S]*' + exact_sub_doc[end_index:], text) \n",
    "        return print('PRESIDENT:'+ result[0]) \n",
    "    except IndexError:\n",
    "        begin_index-=5\n",
    "        \n",
    "    try: \n",
    "        result = re.findall(exact_sub_doc[:begin_index] + '[.\\s\\S]*' + exact_sub_doc[end_index:], text) \n",
    "        return print('PRESIDENT:'+ result[0]) \n",
    "    except IndexError:\n",
    "        end_index+=5\n",
    "        pass\n",
    "    \n",
    "    try: \n",
    "        result = re.findall(exact_sub_doc[:begin_index] + '[.\\s\\S]*' + exact_sub_doc[end_index:], text) \n",
    "        return print('PRESIDENT:'+ result[0]) \n",
    "    except IndexError:\n",
    "        begin_index-=5\n",
    "        pass\n",
    "    \n",
    "    try: \n",
    "        result = re.findall(exact_sub_doc[:begin_index] + '[.\\s\\S]*' + exact_sub_doc[end_index:], text) \n",
    "        return print('PRESIDENT:'+ result[0]) \n",
    "    except IndexError:\n",
    "        end_index+=5\n",
    "        return print('Uh oh, someone erased the tape!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
